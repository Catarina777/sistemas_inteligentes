{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 9: Adapta o KMer para calcular a composição peptídica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1) O KMer deve ser capaz de calcular a composição nucleotídica e peptídica. Podes adicionar um novo parâmetro chamado alphabet onde o utilizador fornece o alfabeto da sequência biológica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2) Testa o novo KMer para sequências de aminoácidos:\n",
    "\n",
    "1. Usa o dataset transporters.csv. Inspeciona o conteúdo do dataset.\n",
    "\n",
    "2. Usa o KMer para obter a frequência de cada substring em cada sequência do dataset. Tamanho da substring (k): 2\n",
    "\n",
    "3. Usa o sklearn.preprocessing.StandardScaler para standardizar o dataset da composição pepetídica. dataset.X = StandardScaler().fit_transform(dataset.X)\n",
    "\n",
    "4. Divide o dataset em treino e teste.\n",
    "\n",
    "5. Treina o modelo LogisticRegression no dataset de composição pepetídica.\n",
    "\n",
    "6. Qual o score obtido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from si.io.csv import read_csv\n",
    "from si.feature_extraction.k_mer import KMer\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.linear_module.logistic_regression import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file to the notebook\n",
    "path = r\"C:/Users/Catarina Ferreira/Desktop/Bioinf 2ºano/sistemas_inteligentes/datasets/tfbs.csv\"\n",
    "# Reading the csv file\n",
    "tfbs = read_csv(path, features=True, label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.]\n",
      "-------------------\n",
      "Score: 94.66666666666667\n"
     ]
    }
   ],
   "source": [
    "k_mer = KMer()\n",
    "tfbs_kmer = k_mer.fit_transform(tfbs)\n",
    "\n",
    "# Standardizing and transforming the values\n",
    "tfbs_kmer.x = StandardScaler().fit_transform(tfbs_kmer.x)\n",
    "# Spliting the dataset in train and testing parts\n",
    "train, test = train_test_split(tfbs_kmer)\n",
    "\n",
    "logistic = LogisticRegression(max_iter=2000)\n",
    "logistic.fit(train)\n",
    "# Printing the predicted values of the test\n",
    "prediction = logistic.predict(test)\n",
    "print(f\"Predictions:\\n\", prediction)\n",
    "print('-------------------')\n",
    "# Printing accuracy of the model \n",
    "scores = logistic.score(test)\n",
    "print(\"Score:\", scores*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file to the notebook\n",
    "path = r\"C:/Users/Catarina Ferreira/Desktop/Bioinf 2ºano/sistemas_inteligentes/datasets/transporters.csv\"\n",
    "# Reading the csv file\n",
    "transporters = read_csv(path, features=True, label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1.]\n",
      "-------------------\n",
      "Score: 79.1044776119403\n"
     ]
    }
   ],
   "source": [
    "kmer_peptide = KMer(alphabet='peptide')\n",
    "transporters_kmer = kmer_peptide.fit_transform(transporters)\n",
    "# Standardizing and transforming the values\n",
    "transporters_kmer.x = StandardScaler().fit_transform(transporters_kmer.x)\n",
    "# Spliting the dataset in train and testing parts\n",
    "train, test = train_test_split(transporters_kmer)\n",
    "\n",
    "logistic = LogisticRegression(max_iter=2000)\n",
    "logistic.fit(train)\n",
    "# Printing the predicted values of the test\n",
    "prediction = logistic.predict(test)\n",
    "print(f\"Predictions:\\n\", prediction)\n",
    "print('-------------------')\n",
    "# Printing accuracy of the model \n",
    "scores = logistic.score(test)\n",
    "print(\"Score:\", scores*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "61bd4cd6a51e1248a16ff8ae8fb80f3268d865b218e328ad68c6b6d874359644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
